name: Run Scraper and Cleaner

on:
  schedule:
    - cron: "0 */5 * * *" # Runs every 5 hours
  workflow_dispatch: # Allows manual trigger

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: "3.9"

      - name: Install Dependencies
        run: pip install -r requirements.txt  # Ensure you have a requirements.txt file

      - name: Run Scraper
        run: python JobsForLebanon_Scraper.py

      - name: Commit and Push the raw data CSV
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add JobsForLebanon.csv
          git commit -m "Update raw job data"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  run-cleaner:
    needs: run-scraper  # Waits for scraper to finish
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: "3.10"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Data Cleaner
        run: python JobsForLebanon_Data_Cleaner.py

      - name: Commit and Push the cleaned data CSV
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add cleaned_jobs.csv
          git commit -m "Update cleaned job data"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
